# app.py
import os, io, zipfile, shutil
import streamlit as st
import numpy as np
import pandas as pd

from VietNam import (  # d√πng l·∫°i c√°c h√†m ƒë√£ refactor trong VietNam.py
    preprocess_image, show_before_after, safe_read_csv, load_dataset,
    build_model, plot_training_history, plot_right_wrong_counts, plot_confusion
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score
import tensorflow as tf


st.set_page_config(page_title="Ch·∫©n ƒëo√°n b√†n ch√¢n", layout="wide")
st.title("ü¶∂ Ch·∫©n ƒëo√°n b√†n ch√¢n (VN) ‚Äì Streamlit")

st.markdown("T·∫£i **CSV** v√† **·∫£nh (ZIP)** r·ªìi b·∫•m **Train**. App s·∫Ω kh√¥ng y√™u c·∫ßu file c√≥ s·∫µn trong repo.")

# Khu v·ª±c upload
csv_file = st.file_uploader("üìÑ T·∫£i l√™n CSV (VietNam.csv)", type=["csv"])
zip_file = st.file_uploader("üóÇÔ∏è T·∫£i l√™n ·∫£nh (ZIP th∆∞ m·ª•c images/VietNam/...)", type=["zip"])

name_col = st.text_input("T√™n c·ªôt ch·ª©a t√™n ·∫£nh", value="t√™n")
label_col = st.text_input("T√™n c·ªôt ch·ª©a nh√£n", value="nh√£n")
num_classes = st.number_input("S·ªë l·ªõp (num_classes)", min_value=2, max_value=20, value=5, step=1)

# N∆°i l√†m vi·ªác t·∫°m th·ªùi
work_dir = "/tmp/dataset"
img_dir = os.path.join(work_dir, "images", "VietNam")
csv_path = os.path.join(work_dir, "data", "VietNam.csv")

# Chu·∫©n b·ªã th∆∞ m·ª•c
os.makedirs(os.path.dirname(csv_path), exist_ok=True)
os.makedirs(img_dir, exist_ok=True)

# Khi ng∆∞·ªùi d√πng upload CSV
if csv_file is not None:
    with open(csv_path, "wb") as f:
        f.write(csv_file.read())
    st.success(f"ƒê√£ l∆∞u CSV v√†o {csv_path}")

# Khi ng∆∞·ªùi d√πng upload ZIP ·∫£nh
if zip_file is not None:
    # X√≥a ·∫£nh c≈© & gi·∫£i n√©n m·ªõi
    if os.path.exists(os.path.join(work_dir, "images")):
        shutil.rmtree(os.path.join(work_dir, "images"))
    os.makedirs(os.path.join(work_dir, "images"), exist_ok=True)

    with zipfile.ZipFile(io.BytesIO(zip_file.read())) as z:
        z.extractall(os.path.join(work_dir, "images"))
    st.success("ƒê√£ gi·∫£i n√©n ·∫£nh v√†o /tmp/dataset/images")

    # T√¨m th∆∞ m·ª•c con VietNam n·∫øu ng∆∞·ªùi d√πng n√©n c·∫£ folder kh√°c t√™n
    # N·∫øu kh√¥ng c√≥ VietNam, c·ªë g·∫Øng ƒëo√°n:
    if not os.path.exists(img_dir):
        # l·∫•y folder ·∫£nh s√¢u nh·∫•t ch·ª©a nhi·ªÅu file .jpg/.png
        candidate = None
        for root, dirs, files in os.walk(os.path.join(work_dir, "images")):
            if any(f.lower().endswith((".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff")) for f in files):
                candidate = root
                break
        if candidate:
            img_dir = candidate
            st.info(f"·∫¢nh t√¨m th·∫•y ·ªü: {img_dir}")

st.divider()

# Tham s·ªë hu·∫•n luy·ªán
col1, col2, col3 = st.columns(3)
with col1:
    epochs = st.number_input("Epochs", 1, 200, 15, 1)  # gi·ªØ th·∫•p cho Cloud
with col2:
    batch = st.number_input("Batch size", 1, 256, 32, 1)
with col3:
    val_split = st.slider("Validation split", 0.05, 0.4, 0.2, 0.05)

train_btn = st.button("üöÄ Train")

if train_btn:
    # Ki·ªÉm tra ƒë·ªß file
    if not os.path.exists(csv_path):
        st.error("Ch∆∞a c√≥ CSV. H√£y upload tr∆∞·ªõc.")
        st.stop()
    if not os.path.exists(img_dir):
        st.error("Ch∆∞a c√≥ th∆∞ m·ª•c ·∫£nh. H√£y upload ZIP ·∫£nh tr∆∞·ªõc.")
        st.stop()

    with st.spinner("ƒêang n·∫°p d·ªØ li·ªáu..."):
        try:
            # d√πng load_dataset t·ª´ VietNam.py
            X, y = load_dataset(csv_path, img_dir, name_col=name_col, label_col=label_col, num_classes=num_classes)
        except Exception as e:
            st.exception(e)
            st.stop()

    y_int = np.argmax(y, axis=1)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y_int
    )

    model = build_model(num_classes=num_classes)

    cb = [
        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
    ]

    with st.spinner("ƒêang hu·∫•n luy·ªán..."):
        history = model.fit(
            X_train, y_train,
            validation_split=val_split,
            epochs=int(epochs),
            batch_size=int(batch),
            callbacks=cb,
            verbose=0
        )

    loss, acc = model.evaluate(X_test, y_test, verbose=0)
    st.success(f"‚úÖ Accuracy (test): **{acc:.2%}**")

    # D·ª± ƒëo√°n & metrics
    y_pred = model.predict(X_test, verbose=0)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = np.argmax(y_test, axis=1)

    precision = precision_score(y_true_classes, y_pred_classes, average=None, zero_division=0)
    recall    = recall_score(y_true_classes, y_pred_classes, average=None, zero_division=0)
    f1        = f1_score(y_true_classes, y_pred_classes, average=None, zero_division=0)

    st.write("**Precision:**", precision)
    st.write("**Recall:**", recall)
    st.write("**F1-score:**", f1)

    # V·∫Ω bi·ªÉu ƒë·ªì
    st.subheader("Bi·ªÉu ƒë·ªì hu·∫•n luy·ªán")
    import matplotlib.pyplot as plt
    fig1 = plt.figure(figsize=(8,3)); 
    # t√°i s·ª≠ d·ª•ng h√†m v·∫Ω trong VietNam.py
    from VietNam import plot_training_history as _pth
    _pth(history)  # s·∫Ω hi·ªán ngay nh·ªù Streamlit hook
    st.pyplot(fig1)

    st.subheader("ƒê√∫ng/Sai theo nh√£n")
    fig2 = plt.figure(figsize=(8,3))
    from VietNam import plot_right_wrong_counts as _prwc
    _prwc(y_true_classes, y_pred_classes, num_classes)
    st.pyplot(fig2)

    st.subheader("Confusion Matrix")
    fig3 = plt.figure(figsize=(5,4))
    from VietNam import plot_confusion as _pc
    _pc(y_true_classes, y_pred_classes, num_classes)
    st.pyplot(fig3)

    # L∆∞u model
    os.makedirs("/app/models", exist_ok=True) if os.path.exists("/app") else os.makedirs("./models", exist_ok=True)
    model_path = "./models/flatfoot_streamlit.keras"
    model.save(model_path)
    st.success(f"üíæ Model saved to `{model_path}`")
