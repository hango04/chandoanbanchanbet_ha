import os
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# 1. H√†m ti·ªÅn x·ª≠ l√Ω ·∫£nh
def preprocess_image(path):
  
    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {path}")
        return None
    img = cv2.equalizeHist(img)
    kernel = np.array([[0, -1, 0],
                       [-1, 5, -1],
                       [0, -1, 0]])
    img = cv2.filter2D(img, -1, kernel)
    img = cv2.resize(img, (224, 224))
    img = img / 255.0
    return img
# H√†m hi·ªÉn th·ªã ·∫£nh g·ªëc v√† ·∫£nh sau x·ª≠ l√Ω
def show_before_after(path):
    original = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    processed = preprocess_image(path)

    if original is None or processed is None:
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ hi·ªÉn th·ªã ·∫£nh.")
        return

    plt.figure(figsize=(8, 4))

    plt.subplot(1, 2, 1)
    plt.imshow(original, cmap='gray')
    plt.title('·∫¢nh g·ªëc')
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(processed, cmap='gray')
    plt.title('·∫¢nh sau x·ª≠ l√Ω')
    plt.axis('off')

    plt.tight_layout()
    plt.show()

#  ƒê∆∞·ªùng d·∫´n ·∫£nh b·∫°n mu·ªën ki·ªÉm tra
image_path = 'd:/banchan/VietNam/021.jpg'  ######################################################## ‚Üê thay b·∫±ng t√™n ·∫£nh th·∫≠t

# G·ªçi h√†m hi·ªÉn th·ªã
show_before_after(image_path)

# 2. ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV v·ªõi x·ª≠ l√Ω m√£ h√≥a v√† t√™n c·ªôt
csv_path = 'd:/banchan/VietNam.csv'                    ####                                           B·ªô  #############
image_folder = 'd:/banchan/VietNam'                    ####              B·ªô                               ############

if not os.path.exists(csv_path):
    raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {csv_path}")

try:
    df = pd.read_csv(csv_path, encoding='utf-8')
except UnicodeDecodeError:
    df = pd.read_csv(csv_path, encoding='latin1')
    print("‚ö†Ô∏è ƒê√£ chuy·ªÉn sang encoding 'latin1' do l·ªói m√£ h√≥a UTF-8")

# Chu·∫©n h√≥a t√™n c·ªôt
df.columns = df.columns.str.strip().str.lower()
print("üìã C√°c c·ªôt trong file:", df.columns.tolist())

# 3. X·ª≠ l√Ω d·ªØ li·ªáu ·∫£nh v√† nh√£n
X = []
y = []

for _, row in df.iterrows():
    filename = row['t√™n']         # d√πng ƒë√∫ng t√™n c·ªôt vi·∫øt th∆∞·ªùng
    label = row['nh√£n']           # n·∫øu c·∫ßn, ƒë·ªïi th√†nh 'nh√£n s·ªë' n·∫øu c·ªôt t√™n v·∫≠y
    path = os.path.join(image_folder, filename)
    img = preprocess_image(path)
    if img is not None:
        X.append(img)
        y.append(label)

X = np.array(X).reshape(-1, 224, 224, 1)
y = to_categorical(y, num_classes=5)

# 4. Chia d·ªØ li·ªáu
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5. X√¢y d·ª±ng m√¥ h√¨nh CNN
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),
    MaxPooling2D(2, 2),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(5, activation='softmax')
])

# 6. Compile v√† hu·∫•n luy·ªán
model.compile(optimizer=Adam(learning_rate=0.00005),  # learning_rate=0.00005 l√† t·ªëc ƒë·ªô h·ªçc.
                 # Gi√° tr·ªã nh·ªè gi√∫p h·ªçc ch·∫≠m nh∆∞ng ch√≠nh x√°c h∆°n, tr√°nh dao ƒë·ªông m·∫°nh.
              loss='categorical_crossentropy',        # H√†m m·∫•t m√°t d√πng ƒë·ªÉ ƒëo sai s·ªë gi·ªØa nh√£n th·∫≠t (y_true) v√† d·ª± ƒëo√°n (y_pred).
                 # 'categorical_crossentropy' d√πng cho b√†i to√°n ph√¢n lo·∫°i nhi·ªÅu l·ªõp (multi-class).
                                                     
              metrics=['accuracy'])                   #ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh (accuracy).


early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
   # Theo d√µi gi√° tr·ªã "loss" tr√™n t·∫≠p validation (d·ªØ li·ªáu ki·ªÉm tra trong qu√° tr√¨nh h·ªçc).
   # N·∫øu sau 5 epoch li√™n ti·∫øp m√† val_loss kh√¥ng gi·∫£m ‚Üí d·ª´ng hu·∫•n luy·ªán s·ªõm.
   # Sau khi d·ª´ng, m√¥ h√¨nh t·ª± ƒë·ªông kh√¥i ph·ª•c tr·ªçng s·ªë t·ªët nh·∫•t (val_loss th·∫•p nh·∫•t).
   # Gi√∫p tr√°nh overfitting

#  model.fit ƒê√¢y l√† h√†m ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh
history = model.fit(X_train, y_train,       #X_train: D·ªØ li·ªáu ƒë·∫ßu v√†o (features). y_train: Nh√£n
                    validation_split=0.2,   # T√°ch 20% d·ªØ li·ªáu ƒë·ªÉ ki·ªÉm tra.
                    #L·ªánh n√†y y√™u c·∫ßu model.fit t·ª± ƒë·ªông d√†nh ra 20% c·ªßa X_train v√† y_train ƒë·ªÉ l√†m d·ªØ li·ªáu ki·ªÉm th·ª≠ (validation set).
                    epochs=60, #S·ªë v√≤ng l·∫∑p hu·∫•n luy·ªán.
                    batch_size=68,
                    callbacks=[early_stop]) # G·∫Øn callback "early_stop" ƒë·ªÉ m√¥ h√¨nh t·ª± ƒë·ªông d·ª´ng khi kh√¥ng c·∫£i thi·ªán.

# 7. ƒê√°nh gi√° m√¥ h√¨nh
loss, acc = model.evaluate(X_test, y_test)
print(f' ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p ki·ªÉm tra: {acc:.2%}')

# 8. L∆∞u m√¥ h√¨nh
#model.save('flatfoot_model_VietNam.keras')
model.save('d:/banchan/flatfoot_model_VietNam.keras')
print(" M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'flatfoot_model_VietNam.keras'")

import matplotlib.pyplot as plt
import numpy as np

# D·ª± ƒëo√°n
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

# T√≠nh s·ªë l∆∞·ª£ng ƒë√∫ng v√† sai theo t·ª´ng nh√£n
num_classes = y_test.shape[1]
correct_counts = np.zeros(num_classes)
incorrect_counts = np.zeros(num_classes)

for true, pred in zip(y_true_classes, y_pred_classes):
    if true == pred:
        correct_counts[true] += 1
    else:
        incorrect_counts[true] += 1

# V·∫Ω bi·ªÉu ƒë·ªì
labels = [f'L·ªõp {i}' for i in range(num_classes)]
x = np.arange(num_classes)
width = 0.35

plt.figure(figsize=(10, 5))
plt.bar(x - width/2, correct_counts, width, label='D·ª± ƒëo√°n ƒë√∫ng', color='mediumseagreen')
plt.bar(x + width/2, incorrect_counts, width, label='D·ª± ƒëo√°n sai', color='tomato')

plt.xticks(x, labels)
plt.ylabel('S·ªë l∆∞·ª£ng m·∫´u')
plt.title('S·ªë l∆∞·ª£ng d·ª± ƒëo√°n ƒë√∫ng v√† sai theo t·ª´ng nh√£n')
plt.legend()
plt.tight_layout()
plt.show()

##########################################################################################################################
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score
import numpy as np

# 1. V·∫Ω bi·ªÉu ƒë·ªì Accuracy v√† Loss
def plot_training_history(history):
    plt.figure(figsize=(10, 4))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy', color='blue')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')
    plt.title('Accuracy over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss', color='green')
    plt.plot(history.history['val_loss'], label='Validation Loss', color='red')
    plt.title('Loss over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

# G·ªçi h√†m v·∫Ω bi·ªÉu ƒë·ªì hu·∫•n luy·ªán
plot_training_history(history)

# 2. V·∫Ω bi·ªÉu ƒë·ªì Precision, Recall, F1-score cho t·ª´ng l·ªõp
# D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

# T√≠nh c√°c ch·ªâ s·ªë
precision = precision_score(y_true_classes, y_pred_classes, average=None)
recall = recall_score(y_true_classes, y_pred_classes, average=None)
f1 = f1_score(y_true_classes, y_pred_classes, average=None)

# V·∫Ω bi·ªÉu ƒë·ªì c·ªôt
labels = [f'L·ªõp {i}' for i in range(len(precision))]
x = np.arange(len(labels))
width = 0.25

plt.figure(figsize=(10, 5))
plt.bar(x - width, precision, width, label='Precision', color='skyblue')
plt.bar(x, recall, width, label='Recall', color='lightgreen')
plt.bar(x + width, f1, width, label='F1-score', color='salmon')

plt.xticks(x, labels)
plt.ylabel('Gi√° tr·ªã')
plt.title('Precision, Recall, F1-score theo t·ª´ng l·ªõp')
plt.legend()
plt.tight_layout()
plt.show()

#######################################################################################################################

# Th√™m 2 th∆∞ vi·ªán n√†y v√†o ƒë·∫ßu file n·∫øu ch∆∞a c√≥
from sklearn.metrics import confusion_matrix
import seaborn as sns

# (ƒêo·∫°n code t√≠nh y_pred_classes v√† y_true_classes c·ªßa b·∫°n ƒë√£ c√≥ s·∫µn)
# y_pred = model.predict(X_test)
# y_pred_classes = np.argmax(y_pred, axis=1)
# y_true_classes = np.argmax(y_test, axis=1)

# 3. V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n (Confusion Matrix)
print("üìä ƒêang t·∫°o ma tr·∫≠n nh·∫ßm l·∫´n...")
cm = confusion_matrix(y_true_classes, y_pred_classes)
class_names = [f'Nh√£n {i}' for i in range(num_classes)] # num_classes = 5

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=class_names, yticklabels=class_names)

plt.title('Ma tr·∫≠n nh·∫ßm l·∫´n (Confusion Matrix)')
plt.ylabel('Nh√£n th·ª±c t·∫ø (Actual)')
plt.xlabel('Nh√£n d·ª± ƒëo√°n (Predicted)')
plt.show()
